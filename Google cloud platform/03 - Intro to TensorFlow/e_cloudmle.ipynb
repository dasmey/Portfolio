{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise\n",
    "\n",
    "Modify your solution to the challenge exercise in d_trainandevaluate.ipynb appropriately. Make sure that you implement training and deployment. Increase the size of your dataset by 10x since you are running on the cloud. Does your accuracy improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'qwiklabs-gcp-bf4d36c16278acd2' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'qwiklabs-gcp-bf4d36c16278acd2' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'europe-west1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Python Code\n",
    "# Model Info\n",
    "MODEL_NAME = 'Cylinder_volume'\n",
    "# Model Version\n",
    "MODEL_VERSION = 'v1'\n",
    "# Training Directory name\n",
    "TRAINING_DIR = 'Cylinder_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Bash Code\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME\n",
    "os.environ['MODEL_VERSION'] = MODEL_VERSION\n",
    "os.environ['TRAINING_DIR'] = TRAINING_DIR \n",
    "os.environ['TFVERSION'] = '1.8'  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the service account email associated with the Cloud Machine Learning Engine API\n",
      "Authorizing the Cloud ML Service account service-679860517096@cloud-ml.google.com.iam.gserviceaccount.com to access files in qwiklabs-gcp-bf4d36c16278acd2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   235    0   235    0     0    244      0 --:--:-- --:--:-- --:--:--   244\r",
      "100   235    0   235    0     0    244      0 --:--:-- --:--:-- --:--:--   244\n",
      "No changes to gs://qwiklabs-gcp-bf4d36c16278acd2/\n",
      "No changes to gs://qwiklabs-gcp-bf4d36c16278acd2/datalab-backups/europe-west1-b/mydatalabvm/content/daily-20181114182409\n",
      "No changes to gs://qwiklabs-gcp-bf4d36c16278acd2/datalab-backups/europe-west1-b/mydatalabvm/content/hourly-20181114182409\n",
      "No changes to gs://qwiklabs-gcp-bf4d36c16278acd2/datalab-backups/europe-west1-b/mydatalabvm/content/weekly-20181114182409\n",
      "No changes to gs://qwiklabs-gcp-bf4d36c16278acd2/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# This command will fail if the Cloud Machine Learning Engine API is not enabled using the link above.\n",
    "echo \"Getting the service account email associated with the Cloud Machine Learning Engine API\"\n",
    "\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "    print (response['serviceAccount'])\")  # If this command fails, the Cloud Machine Learning Engine API has not been enabled above.\n",
    "\n",
    "echo \"Authorizing the Cloud ML Service account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET   \n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET   # error message (if bucket is empty) can be ignored.  \n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cylinder_volume\n",
      "Cylinder_volume/.ipynb_checkpoints\n",
      "Cylinder_volume/trainer\n",
      "Cylinder_volume/trainer/__init__.py\n",
      "Cylinder_volume/trainer/task.py\n",
      "Cylinder_volume/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "find ${MODEL_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\n",
      "# Copyright 2017 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#      http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "from __future__ import absolute_import\n",
      "from __future__ import division\n",
      "from __future__ import print_function\n",
      "\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import shutil\n",
      "\n",
      "tf.logging.set_verbosity(tf.logging.INFO)\n",
      "\n",
      "# List the CSV columns\n",
      "CSV_COLUMNS = ['radius', 'height', 'volume', 'key']\n",
      "\n",
      "#Choose which column is your label\n",
      "LABEL_COLUMN = 'volume'\n",
      "\n",
      "# Set the default values for each CSV column in case there is a missing value\n",
      "DEFAULTS = [[1.0], [1.0], [5.0], ['nokey']]\n",
      "\n",
      "# Create an input function that stores your data into a dataset\n",
      "def read_dataset(filename, mode, batch_size = 512):\n",
      "    def _input_fn():\n",
      "        def decode_csv(value_column):\n",
      "            columns = tf.decode_csv(value_column, record_defaults = DEFAULTS)\n",
      "            features = dict(zip(CSV_COLUMNS, columns))\n",
      "            label = features.pop(LABEL_COLUMN)\n",
      "            return features, label\n",
      "    \n",
      "        # Create list of files that match pattern\n",
      "        file_list = tf.gfile.Glob(filename)\n",
      "\n",
      "        # Create dataset from file list\n",
      "        dataset = tf.data.TextLineDataset(file_list).map(decode_csv)\n",
      "        \n",
      "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
      "            num_epochs = None # indefinitely\n",
      "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
      "        else:\n",
      "            num_epochs = 1 # end-of-input after this\n",
      "\n",
      "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
      "        return dataset.make_one_shot_iterator().get_next()\n",
      "    return _input_fn\n",
      "\n",
      "# Define your feature columns\n",
      "INPUT_COLUMNS = [\n",
      "    tf.feature_column.numeric_column('radius'),\n",
      "    tf.feature_column.numeric_column('height'),\n",
      "]\n",
      "\n",
      "# Create a function that will augment your feature set\n",
      "def add_more_features(feats):\n",
      "    # Nothing to add (yet!)\n",
      "    return feats\n",
      "\n",
      "feature_cols = add_more_features(INPUT_COLUMNS)\n",
      "\n",
      "# Create your serving input function so that your trained model will be able to serve predictions\n",
      "def serving_input_fn():\n",
      "    feature_placeholders = {\n",
      "        column.name: tf.placeholder(tf.float32, [None]) for column in INPUT_COLUMNS\n",
      "    }\n",
      "\n",
      "    features = feature_placeholders\n",
      "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
      "\n",
      "# Create an estimator that we are going to train and evaluate\n",
      "def train_and_evaluate(args):\n",
      "    estimator = tf.estimator.DNNRegressor(\n",
      "        model_dir = args['output_dir'],\n",
      "        feature_columns = feature_cols,\n",
      "        hidden_units = args['hidden_units'])\n",
      "    train_spec = tf.estimator.TrainSpec(\n",
      "        input_fn = read_dataset(args['train_data_paths'],\n",
      "                                batch_size = args['train_batch_size'],\n",
      "                                mode = tf.estimator.ModeKeys.TRAIN),\n",
      "        max_steps = args['train_steps'])\n",
      "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
      "    eval_spec = tf.estimator.EvalSpec(\n",
      "        input_fn = read_dataset(args['eval_data_paths'],\n",
      "                                batch_size = 10000,\n",
      "                                mode = tf.estimator.ModeKeys.EVAL),\n",
      "        steps = None,\n",
      "        start_delay_secs = args['eval_delay_secs'],\n",
      "        throttle_secs = args['throttle_secs'],\n",
      "        exporters = exporter)\n",
      "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ${MODEL_NAME}/trainer/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory: /content/datalab/training-data-analyst/courses/machine_learning/deepdive/03_tensorflow\n",
      "Head of df_train.csv\n",
      "0,1.1,1.7,6.3\n",
      "Head of df_valid.csv\n",
      "0,1.7,0.6,5.4\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"Working Directory: ${PWD}\"\n",
    "echo \"Head of df_train.csv\"\n",
    "head -1 $PWD/df_train.csv\n",
    "echo \"Head of df_valid.csv\"\n",
    "head -1 $PWD/df_valid.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwiklabs-gcp-bf4d36c16278acd2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Copying file:///content/datalab/training-data-analyst/courses/machine_learning/deepdive/03_tensorflow/df_valid.csv [Content-Type=text/csv]...\n",
      "Copying file:///content/datalab/training-data-analyst/courses/machine_learning/deepdive/03_tensorflow/taxi-test.csv [Content-Type=text/csv]...\n",
      "/ [0 files][    0.0 B/167.3 KiB]                                                \r",
      "/ [0 files][    0.0 B/253.8 KiB]                                                \r",
      "Copying file:///content/datalab/training-data-analyst/courses/machine_learning/deepdive/03_tensorflow/taxi-train.csv [Content-Type=text/csv]...\n",
      "/ [0 files][    0.0 B/661.3 KiB]                                                \r",
      "Copying file:///content/datalab/training-data-analyst/courses/machine_learning/deepdive/03_tensorflow/df_train.csv [Content-Type=text/csv]...\n",
      "/ [0 files][    0.0 B/ 18.9 MiB]                                                \r",
      "Copying file:///content/datalab/training-data-analyst/courses/machine_learning/deepdive/03_tensorflow/taxi-valid.csv [Content-Type=text/csv]...\n",
      "/ [0/5 files][    0.0 B/ 19.0 MiB]   0% Done                                    \r",
      "/ [1/5 files][  7.7 MiB/ 19.0 MiB]  40% Done                                    \r",
      "/ [2/5 files][  9.0 MiB/ 19.0 MiB]  47% Done                                    \r",
      "/ [3/5 files][  9.5 MiB/ 19.0 MiB]  50% Done                                    \r",
      "/ [4/5 files][  9.8 MiB/ 19.0 MiB]  51% Done                                    \r",
      "/ [5/5 files][ 19.0 MiB/ 19.0 MiB] 100% Done                                    \r",
      "-\r\n",
      "Operation completed over 5 objects/19.0 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Clear Cloud Storage bucket and copy the CSV files to Cloud Storage bucket\n",
    "echo $BUCKET\n",
    "gsutil -m rm -rf gs://${BUCKET}/${MODEL_NAME}/smallinput/\n",
    "gsutil -m cp ${PWD}/*.csv gs://${BUCKET}/${MODEL_NAME}/smallinput/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-bf4d36c16278acd2/Cylinder_volume/smallinput/Cylinder_trained europe-west1 Cylinder_volume_181114_184024\n",
      "jobId: Cylinder_volume_181114_184024\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [Cylinder_volume_181114_184024] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe Cylinder_volume_181114_184024\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs Cylinder_volume_181114_184024\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/${MODEL_NAME}/smallinput/${TRAINING_DIR}\n",
    "JOBNAME=${MODEL_NAME}_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "# Clear the Cloud Storage Bucket used for the training job\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/${MODEL_NAME}/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC \\\n",
    "   --runtime-version=$TFVERSION \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/${MODEL_NAME}/smallinput/df_train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/${MODEL_NAME}/smallinput/df_valid*\"  \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --train_steps=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-bf4d36c16278acd2/Cylinder_volume/smallinput/Cylinder_trained/export/exporter/\n",
      "gs://qwiklabs-gcp-bf4d36c16278acd2/Cylinder_volume/smallinput/Cylinder_trained/export/exporter/1542220962/\n",
      "gs://qwiklabs-gcp-bf4d36c16278acd2/Cylinder_volume/smallinput/Cylinder_trained/export/exporter/1542221345/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/${MODEL_NAME}/smallinput/${TRAINING_DIR}/export/exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created ml engine model [projects/qwiklabs-gcp-bf4d36c16278acd2/models/Cylinder_volume].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_LOCATION = gs://qwiklabs-gcp-bf4d36c16278acd2/Cylinder_volume/smallinput/Cylinder_trained/export/exporter/1542221345/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating version (this might take a few minutes)......\n",
      "..................................................................................................................................................................................................................................................................................................................................................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/${MODEL_NAME}/smallinput/${TRAINING_DIR}/export/exporter | tail -1)\n",
    "\n",
    "echo \"MODEL_LOCATION = ${MODEL_LOCATION}\"\n",
    "\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version $TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./test.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./test.json\n",
    "{\"radius\": 1.2,\"height\": 1.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS\n",
      "[1.1839938163757324]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ml-engine predict --model=${MODEL_NAME} --version=${MODEL_VERSION} --json-instances=./test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Model : step 1 - remove version info \n",
    "Before an existing cloud model can be removed, it must have any version info removed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete model: step 2 - remove existing model\n",
    "Now that the version info is removed from an existing model, the actual model can be removed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ml-engine models delete ${MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
